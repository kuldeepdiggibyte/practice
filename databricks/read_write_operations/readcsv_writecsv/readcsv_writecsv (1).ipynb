{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea63d95d-167d-402b-838d-2120124be46f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n| id|zipcode|    type|               city|state|population|\n+---+-------+--------+-------------------+-----+----------+\n|  1|    704|STANDARD|               null|   PR|     30100|\n|  2|    704|    null|PASEO COSTA DEL SUR|   PR|      null|\n|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n|  5|  76177|STANDARD|               null|   TX|      null|\n+---+-------+--------+-------------------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(path = \"dbfs:/FileStore/tables/small_zipcode.csv\", header=True)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a7d528b-b6d9-4528-afb3-ef82585287ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+-------------------+-----+----------+\n|  id|zipcode|    type|               city|state|population|\n+----+-------+--------+-------------------+-----+----------+\n|null|   null|    type|               city|state|population|\n|   1|    704|STANDARD|               null|   PR|     30100|\n|   2|    704|    null|PASEO COSTA DEL SUR|   PR|      null|\n|   3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n|   4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n|   5|  76177|STANDARD|               null|   TX|      null|\n+----+-------+--------+-------------------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType \n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id',IntegerType()),\n",
    "    StructField('zipcode',IntegerType()),\n",
    "    StructField('type',StringType()),\n",
    "    StructField('city',StringType()),\n",
    "    StructField('state',StringType()),\n",
    "    StructField('population',StringType())\n",
    "])\n",
    "\n",
    "data1 =  spark.read.csv(path = \"dbfs:/FileStore/tables/small_zipcode.csv\",schema = schema)\n",
    "\n",
    "data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd23329a-9002-4254-abd0-bffe0dfb7cf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataFrameWriter in module pyspark.sql.readwriter:\n\nclass DataFrameWriter(OptionUtils)\n |  DataFrameWriter(df: 'DataFrame')\n |  \n |  Interface used to write a :class:`DataFrame` to external storage systems\n |  (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`\n |  to access this.\n |  \n |  .. versionadded:: 1.4.0\n |  \n |  .. versionchanged:: 3.4.0\n |      Support Spark Connect.\n |  \n |  Method resolution order:\n |      DataFrameWriter\n |      OptionUtils\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, df: 'DataFrame')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  bucketBy(self, numBuckets: int, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n |      Buckets the output by the given columns. If specified,\n |      the output is laid out on the file system similar to Hive's bucketing scheme,\n |      but with a different bucket hash function and is not compatible with Hive's bucketing.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      numBuckets : int\n |          the number of buckets to save\n |      col : str, list or tuple\n |          a name of a column, or a list of names.\n |      cols : str\n |          additional names (optional). If `col` is a list it should be empty.\n |      \n |      Notes\n |      -----\n |      Applicable for file-based data sources in combination with\n |      :py:meth:`DataFrameWriter.saveAsTable`.\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file in a buckted manner, and read it back.\n |      \n |      >>> from pyspark.sql.functions import input_file_name\n |      >>> # Write a DataFrame into a Parquet file in a bucketed manner.\n |      ... _ = spark.sql(\"DROP TABLE IF EXISTS bucketed_table\")\n |      >>> spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... ).write.bucketBy(2, \"name\").mode(\"overwrite\").saveAsTable(\"bucketed_table\")\n |      >>> # Read the Parquet file as a DataFrame.\n |      ... spark.read.table(\"bucketed_table\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE bucketed_table\")\n |  \n |  csv(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n |              exists.\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a CSV file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file\n |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n |      ...     df.write.csv(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n |      ...     spark.read.schema(df.schema).format(\"csv\").option(\n |      ...         \"nullValue\", \"Hyukjin Kwon\").load(d).show()\n |      +---+----+\n |      |age|name|\n |      +---+----+\n |      |100|null|\n |      +---+----+\n |  \n |  format(self, source: str) -> 'DataFrameWriter'\n |      Specifies the underlying output data source.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      source : str\n |          string, name of the data source, e.g. 'json', 'parquet'.\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1).write.format('parquet')\n |      <...readwriter.DataFrameWriter object ...>\n |      \n |      Write a DataFrame into a Parquet file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.format('parquet').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  insertInto(self, tableName: str, overwrite: Optional[bool] = None) -> None\n |      Inserts the content of the :class:`DataFrame` to the specified table.\n |      \n |      It requires that the schema of the :class:`DataFrame` is the same as the\n |      schema of the table.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      overwrite : bool, optional\n |          If true, overwrites existing data. Disabled by default\n |      \n |      Notes\n |      -----\n |      Unlike :meth:`DataFrameWriter.saveAsTable`, :meth:`DataFrameWriter.insertInto` ignores\n |      the column names and just uses position-based resolution.\n |      \n |      Examples\n |      --------\n |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n |      >>> df = spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... )\n |      >>> df.write.saveAsTable(\"tblA\")\n |      \n |      Insert the data into 'tblA' table but with different column names.\n |      \n |      >>> df.selectExpr(\"age AS col1\", \"name AS col2\").write.insertInto(\"tblA\")\n |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n |  \n |  jdbc(self, url: str, table: str, mode: Optional[str] = None, properties: Optional[Dict[str, str]] = None) -> None\n |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      table : str\n |          Name of the table in the external database.\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      properties : dict\n |          a dictionary of JDBC database connection arguments. Normally at\n |          least properties \"user\" and \"password\" with their corresponding values.\n |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Notes\n |      -----\n |      Don't create too many partitions in parallel on a large cluster;\n |      otherwise Spark might crash your external database systems.\n |  \n |  json(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, lineSep: Optional[str] = None, encoding: Optional[str] = None, ignoreNullFields: Union[bool, str, NoneType] = None) -> None\n |      Saves the content of the :class:`DataFrame` in JSON format\n |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n |      specified path.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a JSON file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a JSON file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.json(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the JSON file as a DataFrame.\n |      ...     spark.read.format(\"json\").load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  mode(self, saveMode: Optional[str]) -> 'DataFrameWriter'\n |      Specifies the behavior when data or table already exists.\n |      \n |      Options include:\n |      \n |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n |      * `overwrite`: Overwrite existing data.\n |      * `error` or `errorifexists`: Throw an exception if data already exists.\n |      * `ignore`: Silently ignore this operation if data already exists.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Examples\n |      --------\n |      Raise an error when writing to an existing path.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 80, \"name\": \"Xinrong Meng\"}]\n |      ...     ).write.mode(\"error\").format(\"parquet\").save(d) # doctest: +SKIP\n |      Traceback (most recent call last):\n |          ...\n |      ...AnalysisException: ...\n |      \n |      Write a Parquet file back with various options, and read it back.\n |      \n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Overwrite the path with a new Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Append another DataFrame into the Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 120, \"name\": \"Takuya Ueshin\"}]\n |      ...     ).write.mode(\"append\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Append another DataFrame into the Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 140, \"name\": \"Haejoon Lee\"}]\n |      ...     ).write.mode(\"ignore\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.parquet(d).show()\n |      +---+-------------+\n |      |age|         name|\n |      +---+-------------+\n |      |120|Takuya Ueshin|\n |      |100| Hyukjin Kwon|\n |      +---+-------------+\n |  \n |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n |      Adds an output option for the underlying data source.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      key : str\n |          The key for the option to set.\n |      value\n |          The value for the option to set.\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1).write.option(\"key\", \"value\")\n |      <...readwriter.DataFrameWriter object ...>\n |      \n |      Specify the option 'nullValue' with writing a CSV file.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon'.\n |      ...     df = spark.createDataFrame([(100, None)], \"age INT, name STRING\")\n |      ...     df.write.option(\"nullValue\", \"Hyukjin Kwon\").mode(\"overwrite\").format(\"csv\").save(d)\n |      ...\n |      ...     # Read the CSV file as a DataFrame.\n |      ...     spark.read.schema(df.schema).format('csv').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n |      Adds output options for the underlying data source.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      **options : dict\n |          The dictionary of string keys and primitive-type values.\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1).write.option(\"key\", \"value\")\n |      <...readwriter.DataFrameWriter object ...>\n |      \n |      Specify the option 'nullValue' and 'header' with writing a CSV file.\n |      \n |      >>> from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n |      >>> schema = StructType([\n |      ...     StructField(\"age\",IntegerType(),True),\n |      ...     StructField(\"name\",StringType(),True),\n |      ... ])\n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon',\n |      ...     # and 'header' option set to `True`.\n |      ...     df = spark.createDataFrame([(100, None)], schema=schema)\n |      ...     df.write.options(nullValue=\"Hyukjin Kwon\", header=True).mode(\n |      ...         \"overwrite\").format(\"csv\").save(d)\n |      ...\n |      ...     # Read the CSV file as a DataFrame.\n |      ...     spark.read.option(\"header\", True).format('csv').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  orc(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : str or list, optional\n |          names of partitioning columns\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a ORC file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a ORC file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.orc(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.format(\"orc\").load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  parquet(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : str or list, optional\n |          names of partitioning columns\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.parquet(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.format(\"parquet\").load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  partitionBy(self, *cols: Union[str, List[str]]) -> 'DataFrameWriter'\n |      Partitions the output by the given columns on the file system.\n |      \n |      If specified, the output is laid out on the file system similar\n |      to Hive's partitioning scheme.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      cols : str or list\n |          name of columns\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file in a partitioned manner, and read it back.\n |      \n |      >>> import tempfile\n |      >>> import os\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file in a partitioned manner.\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}, {\"age\": 120, \"name\": \"Ruifeng Zheng\"}]\n |      ...     ).write.partitionBy(\"name\").mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.parquet(d).sort(\"age\").show()\n |      ...\n |      ...     # Read one partition as a DataFrame.\n |      ...     spark.read.parquet(f\"{d}{os.path.sep}name=Hyukjin Kwon\").show()\n |      +---+-------------+\n |      |age|         name|\n |      +---+-------------+\n |      |100| Hyukjin Kwon|\n |      |120|Ruifeng Zheng|\n |      +---+-------------+\n |      +---+\n |      |age|\n |      +---+\n |      |100|\n |      +---+\n |  \n |  save(self, path: Optional[str] = None, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n |      Saves the contents of the :class:`DataFrame` to a data source.\n |      \n |      The data source is specified by the ``format`` and a set of ``options``.\n |      If ``format`` is not specified, the default data source configured by\n |      ``spark.sql.sources.default`` will be used.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str, optional\n |          the path in a Hadoop supported file system\n |      format : str, optional\n |          the format used to save\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : list, optional\n |          names of partitioning columns\n |      **options : dict\n |          all other string options\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a JSON file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a JSON file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n |      ...\n |      ...     # Read the JSON file as a DataFrame.\n |      ...     spark.read.format('json').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  saveAsTable(self, name: str, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n |      Saves the content of the :class:`DataFrame` as the specified table.\n |      \n |      In the case the table already exists, behavior of this function depends on the\n |      save mode, specified by the `mode` function (default to throwing an exception).\n |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n |      the same as that of the existing table.\n |      \n |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n |      * `overwrite`: Overwrite existing data.\n |      * `error` or `errorifexists`: Throw an exception if data already exists.\n |      * `ignore`: Silently ignore this operation if data already exists.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Notes\n |      -----\n |      When `mode` is `Append`, if there is an existing table, we will use the format and\n |      options of the existing table. The column order in the schema of the :class:`DataFrame`\n |      doesn't need to be the same as that of the existing table. Unlike\n |      :meth:`DataFrameWriter.insertInto`, :meth:`DataFrameWriter.saveAsTable` will use the\n |      column names to find the correct column positions.\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          the table name\n |      format : str, optional\n |          the format used to save\n |      mode : str, optional\n |          one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n |      partitionBy : str or list\n |          names of partitioning columns\n |      **options : dict\n |          all other string options\n |      \n |      Examples\n |      --------\n |      Creates a table from a DataFrame, and read it back.\n |      \n |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n |      >>> spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... ).write.saveAsTable(\"tblA\")\n |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n |  \n |  sortBy(self, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n |      Sorts the output in each bucket by the given columns on the file system.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      col : str, tuple or list\n |          a name of a column, or a list of names.\n |      cols : str\n |          additional names (optional). If `col` is a list it should be empty.\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file in a sorted-buckted manner, and read it back.\n |      \n |      >>> from pyspark.sql.functions import input_file_name\n |      >>> # Write a DataFrame into a Parquet file in a sorted-bucketed manner.\n |      ... _ = spark.sql(\"DROP TABLE IF EXISTS sorted_bucketed_table\")\n |      >>> spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... ).write.bucketBy(1, \"name\").sortBy(\"age\").mode(\n |      ...     \"overwrite\").saveAsTable(\"sorted_bucketed_table\")\n |      >>> # Read the Parquet file as a DataFrame.\n |      ... spark.read.table(\"sorted_bucketed_table\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE sorted_bucketed_table\")\n |  \n |  text(self, path: str, compression: Optional[str] = None, lineSep: Optional[str] = None) -> None\n |      Saves the content of the DataFrame in a text file at the specified path.\n |      The text files will be encoded as UTF-8.\n |      \n |      .. versionadded:: 1.6.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Notes\n |      -----\n |      The DataFrame must have only one column that is of string type.\n |      Each row becomes a new line in the output file.\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a text file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a text file\n |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n |      ...     df.write.mode(\"overwrite\").text(d)\n |      ...\n |      ...     # Read the text file as a DataFrame.\n |      ...     spark.read.schema(df.schema).format(\"text\").load(d).sort(\"alphabets\").show()\n |      +---------+\n |      |alphabets|\n |      +---------+\n |      |        a|\n |      |        b|\n |      |        c|\n |      +---------+\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from OptionUtils:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "help(DataFrameWriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606d6e31-05b1-49c5-93fb-882f160f627a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n| id|   name|Salary|\n+---+-------+------+\n|  1|Kuldeep| 30000|\n|  2|pralhad| 40000|\n|  3| kartik| 50000|\n+---+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [(1,\"Kuldeep\",30000),\n",
    "        (2,\"pralhad\",40000),\n",
    "        (3,\"kartik\",50000)]\n",
    "schema = StructType([\n",
    "    StructField(\"id\",IntegerType()),\n",
    "    StructField(\"name\",StringType()),\n",
    "    StructField(\"Salary\",IntegerType())\n",
    "])\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e65557e-97f2-4686-a63b-52363f716444",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1911129993818233>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdbfs:/temp/emps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n",
       "\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n",
       "\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n",
       "\u001B[1;32m   1797\u001B[0m )\n",
       "\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/temp/emps already exists."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1911129993818233>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdbfs:/temp/emps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m   1797\u001B[0m )\n\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/temp/emps already exists.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Path dbfs:/temp/emps already exists.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.csv(path=\"dbfs:/temp/emps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f22df5-3841-4cb1-a772-c34f53e6fcfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n| id|   name|Salary|\n+---+-------+------+\n|  1|Kuldeep| 30000|\n|  2|pralhad| 40000|\n|  3| kartik| 50000|\n+---+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(path=\"dbfs:/temp/emps_header\",header=True)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48208923-6159-4bb9-ad18-606ae86c25c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n| id|   name|Salary|\n+---+-------+------+\n|  1|Kuldeep| 30000|\n|  2|pralhad| 40000|\n|  3| kartik| 50000|\n+---+-------+------+\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.format('csv').option('header',True).option('inferschema',True).load(\"dbfs:/temp/emps_header\")\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec0089be-743b-458b-baa0-027c2d15f2d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n| id|   name|Salary|\n+---+-------+------+\n|  1|Kuldeep| 30000|\n|  2|pralhad| 40000|\n|  3| kartik| 50000|\n+---+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.write.csv(path=\"dbfs:/temp/emps_header1\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67555ddd-359f-4304-b567-2bcd975fb27d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n| id|    name|age|\n+---+--------+---+\n|  1|pratibha| 20|\n|  2| krishna| 22|\n|  3|   manvi| 23|\n|  4| kuldeep| 21|\n|  5|   kalai| 22|\n+---+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [(1,\"pratibha\",20),\n",
    "        (2,\"krishna\",22),\n",
    "        (3,\"manvi\",23),\n",
    "        (4,\"kuldeep\",21),\n",
    "        (5,\"kalai\",22)]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id',IntegerType()),\n",
    "    StructField('name',StringType()),\n",
    "    StructField('age',IntegerType())\n",
    "]) \n",
    "df2 = spark.createDataFrame(data=data,schema=schema)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9868bbf7-158f-4250-bf1f-dc905d578006",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.write.csv(path=\"dbfs:/temp/emps_header\",header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a799e47-d58e-4ab8-add8-c66c7dc3b7d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This below cell contain overwritten data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067c04d8-6801-4bda-a58e-09ec8da2511a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n| id|    name|age|\n+---+--------+---+\n|  1|pratibha| 20|\n|  2| krishna| 22|\n|  4| kuldeep| 21|\n|  3|   manvi| 23|\n|  5|   kalai| 22|\n+---+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(path='dbfs:/temp/emps_header',header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d305ad-c173-44cf-8e41-c6a4733dc16c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1911129993818238>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdbfs:/temp/emps_header\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43merror\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n",
       "\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n",
       "\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n",
       "\u001B[1;32m   1797\u001B[0m )\n",
       "\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/temp/emps_header already exists."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1911129993818238>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdbfs:/temp/emps_header\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43merror\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m   1797\u001B[0m )\n\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/temp/emps_header already exists.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Path dbfs:/temp/emps_header already exists.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.csv(path=\"dbfs:/temp/emps_header\",header=True,mode='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc03db1-1905-49e7-9f44-9b3d8f99edf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.csv(path=\"dbfs:/temp/emps_header\",header=True,mode='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7d90fcc-9326-4cd1-89f4-658a7fd6834d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "readcsv_writecsv",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
